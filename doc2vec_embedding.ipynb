{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "from gensim import utils\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pyfastx\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import smart_open\n",
    "import time\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a sequence, returns a list of kmers.\n",
    "def process_seq(seq, k):\n",
    "    index = 0\n",
    "    out = \"\"\n",
    "    while index < len(seq) - k:\n",
    "        out += \"{0} \".format(seq[index: index + k])\n",
    "        index += 1\n",
    "    out += seq[index:index+k]\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints epochs for gensim models.\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.time = None\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        self.time = time.time()\n",
    "    def on_epoch_end(self, model):\n",
    "        e = time.time() - self.time\n",
    "        print(\"Epoch #{0} end in {1}\".format(self.epoch, e))\n",
    "        self.epoch += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for making \"document\" files from dna seqs.\n",
    "# Document files are one-sequence-per-line text files, where sequences are white-space seperated lists of words.\n",
    "# Words may be k-mers or BPE tokens.\n",
    "\n",
    "### MACROS ###\n",
    "write_kmers = False  #These booleans are here as a stop gap to avoid accidentally overwriting files later.\n",
    "write_bpe = False\n",
    "K = [6,8]\n",
    "\n",
    "SP_FILE = '{}mer_compare_bpe_dna_wordsize_256.model'\n",
    "FASTA_FILE = 'silva_nr_ref_unambiguous_as_dna.fasta'\n",
    "##############\n",
    "\n",
    "for k in K:\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(SP_FILE.format(k))\n",
    "\n",
    "    fa = pyfastx.Fasta(FASTA_FILE)\n",
    "\n",
    "    if write_kmers:\n",
    "        prog = IntProgress(min=0, max=len(fa))\n",
    "        display(prog)\n",
    "\n",
    "        with open('all_16s_as_{0}mer_documents_no_embedding.txt'.format(k), 'w') as outfile:\n",
    "            for record in fa:\n",
    "                outfile.write(\"\\n{}\".format(process_seq(record.seq, k)))\n",
    "                prog.value += 1\n",
    "\n",
    "    if write_bpe:\n",
    "        fa = pyfastx.Fasta(FASTA_FILE)\n",
    "        prog = IntProgress(min=0, max=len(fa))\n",
    "        display(prog)\n",
    "        with open('all_16s_as_{0}mer_bpe_ws256_documents.txt'.format(k), 'w') as outfile:\n",
    "            for record in fa:\n",
    "                outfile.write(\"\\n{}\".format(\" \".join(sp.encode_as_pieces(record.seq))))\n",
    "                prog.value += 1\n",
    "\n",
    "    del(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab....\n",
      "Vocab built in 140.50938034057617\n",
      "Training...\n",
      "Epoch #0 start\n",
      "Epoch #0 end in 250.5376753807068\n",
      "Epoch #1 start\n",
      "Epoch #1 end in 245.91381812095642\n",
      "Epoch #2 start\n",
      "Epoch #2 end in 248.51491808891296\n",
      "Epoch #3 start\n",
      "Epoch #3 end in 254.4101481437683\n",
      "Epoch #4 start\n",
      "Epoch #4 end in 253.2412929534912\n",
      "Epoch #5 start\n",
      "Epoch #5 end in 253.78625392913818\n",
      "Epoch #6 start\n",
      "Epoch #6 end in 258.1486859321594\n",
      "Epoch #7 start\n",
      "Epoch #7 end in 248.08066749572754\n",
      "Epoch #8 start\n",
      "Epoch #8 end in 255.18641805648804\n",
      "Epoch #9 start\n",
      "Epoch #9 end in 252.50441336631775\n",
      "Epoch #10 start\n",
      "Epoch #10 end in 247.39678764343262\n",
      "Epoch #11 start\n",
      "Epoch #11 end in 248.78888750076294\n",
      "Epoch #12 start\n",
      "Epoch #12 end in 251.53982615470886\n",
      "Epoch #13 start\n",
      "Epoch #13 end in 254.1322901248932\n",
      "Epoch #14 start\n",
      "Epoch #14 end in 251.07536005973816\n",
      "Epoch #15 start\n",
      "Epoch #15 end in 253.61028170585632\n",
      "Epoch #16 start\n",
      "Epoch #16 end in 248.05555963516235\n",
      "Epoch #17 start\n",
      "Epoch #17 end in 254.22837257385254\n",
      "Epoch #18 start\n",
      "Epoch #18 end in 261.1869547367096\n",
      "Epoch #19 start\n",
      "Epoch #19 end in 253.10959267616272\n",
      "done\n",
      "Building Vocab....\n",
      "Vocab built in 77.4905834197998\n",
      "Training...\n",
      "Epoch #0 start\n",
      "Epoch #0 end in 59.49447154998779\n",
      "Epoch #1 start\n",
      "Epoch #1 end in 61.11266732215881\n",
      "Epoch #2 start\n",
      "Epoch #2 end in 62.41863965988159\n",
      "Epoch #3 start\n",
      "Epoch #3 end in 62.192015647888184\n",
      "Epoch #4 start\n",
      "Epoch #4 end in 60.437232971191406\n",
      "Epoch #5 start\n",
      "Epoch #5 end in 58.847830295562744\n",
      "Epoch #6 start\n",
      "Epoch #6 end in 60.2353310585022\n",
      "Epoch #7 start\n",
      "Epoch #7 end in 60.34574294090271\n",
      "Epoch #8 start\n",
      "Epoch #8 end in 59.29876089096069\n",
      "Epoch #9 start\n",
      "Epoch #9 end in 59.51620125770569\n",
      "Epoch #10 start\n",
      "Epoch #10 end in 60.649967193603516\n",
      "Epoch #11 start\n",
      "Epoch #11 end in 61.291945934295654\n",
      "Epoch #12 start\n",
      "Epoch #12 end in 60.754270792007446\n",
      "Epoch #13 start\n",
      "Epoch #13 end in 61.073137044906616\n",
      "Epoch #14 start\n",
      "Epoch #14 end in 61.55350184440613\n",
      "Epoch #15 start\n",
      "Epoch #15 end in 61.41600275039673\n",
      "Epoch #16 start\n",
      "Epoch #16 end in 61.576324462890625\n",
      "Epoch #17 start\n",
      "Epoch #17 end in 61.508835554122925\n",
      "Epoch #18 start\n",
      "Epoch #18 end in 61.237433671951294\n",
      "Epoch #19 start\n",
      "Epoch #19 end in 60.84713530540466\n",
      "done\n",
      "Building Vocab....\n",
      "Vocab built in 181.7313175201416\n",
      "Training...\n",
      "Epoch #0 start\n",
      "Epoch #0 end in 439.88523387908936\n",
      "Epoch #1 start\n",
      "Epoch #1 end in 431.0125403404236\n",
      "Epoch #2 start\n",
      "Epoch #2 end in 400.8572909832001\n",
      "Epoch #3 start\n",
      "Epoch #3 end in 400.3240966796875\n",
      "Epoch #4 start\n",
      "Epoch #4 end in 399.3155450820923\n",
      "Epoch #5 start\n",
      "Epoch #5 end in 400.0152225494385\n",
      "Epoch #6 start\n",
      "Epoch #6 end in 399.7514691352844\n",
      "Epoch #7 start\n",
      "Epoch #7 end in 399.70838952064514\n",
      "Epoch #8 start\n",
      "Epoch #8 end in 399.87816071510315\n",
      "Epoch #9 start\n",
      "Epoch #9 end in 399.4735097885132\n",
      "Epoch #10 start\n",
      "Epoch #10 end in 399.45203709602356\n",
      "Epoch #11 start\n",
      "Epoch #11 end in 398.9668550491333\n",
      "Epoch #12 start\n",
      "Epoch #12 end in 399.84058690071106\n",
      "Epoch #13 start\n",
      "Epoch #13 end in 400.0536963939667\n",
      "Epoch #14 start\n",
      "Epoch #14 end in 399.7532911300659\n",
      "Epoch #15 start\n",
      "Epoch #15 end in 399.7845039367676\n",
      "Epoch #16 start\n",
      "Epoch #16 end in 399.39690470695496\n",
      "Epoch #17 start\n",
      "Epoch #17 end in 399.27182483673096\n",
      "Epoch #18 start\n",
      "Epoch #18 end in 399.70336747169495\n",
      "Epoch #19 start\n",
      "Epoch #19 end in 398.78351974487305\n",
      "done\n",
      "Building Vocab....\n",
      "Vocab built in 83.07444477081299\n",
      "Training...\n",
      "Epoch #0 start\n",
      "Epoch #0 end in 65.58674263954163\n",
      "Epoch #1 start\n",
      "Epoch #1 end in 65.02898526191711\n",
      "Epoch #2 start\n",
      "Epoch #2 end in 64.97560906410217\n",
      "Epoch #3 start\n",
      "Epoch #3 end in 65.13256168365479\n",
      "Epoch #4 start\n",
      "Epoch #4 end in 65.22114634513855\n",
      "Epoch #5 start\n",
      "Epoch #5 end in 65.20886468887329\n",
      "Epoch #6 start\n",
      "Epoch #6 end in 65.36851811408997\n",
      "Epoch #7 start\n",
      "Epoch #7 end in 65.34092283248901\n",
      "Epoch #8 start\n",
      "Epoch #8 end in 65.28799939155579\n",
      "Epoch #9 start\n",
      "Epoch #9 end in 65.28538107872009\n",
      "Epoch #10 start\n",
      "Epoch #10 end in 65.25271272659302\n",
      "Epoch #11 start\n",
      "Epoch #11 end in 65.39464354515076\n",
      "Epoch #12 start\n",
      "Epoch #12 end in 65.37587428092957\n",
      "Epoch #13 start\n",
      "Epoch #13 end in 65.453134059906\n",
      "Epoch #14 start\n",
      "Epoch #14 end in 65.52345490455627\n",
      "Epoch #15 start\n",
      "Epoch #15 end in 65.35883164405823\n",
      "Epoch #16 start\n",
      "Epoch #16 end in 65.3421516418457\n",
      "Epoch #17 start\n",
      "Epoch #17 end in 65.38119840621948\n",
      "Epoch #18 start\n",
      "Epoch #18 end in 65.39180731773376\n",
      "Epoch #19 start\n",
      "Epoch #19 end in 65.48853087425232\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for k in K:\n",
    "    n_words = 4 ** k\n",
    "    args = {'dm':0, \n",
    "            'vector_size': 128, \n",
    "            'epochs':20, \n",
    "            'min_count':1, \n",
    "            'sample': 0.0001, \n",
    "            'workers':multiprocessing.cpu_count()}\n",
    "\n",
    "\n",
    "\n",
    "    trials = [('all_16s_as_{0}mer_documents_no_embedding.txt'.format(k), 'doc2vec_{}mers_128dim.model'.format(k)),\n",
    "              ('all_16s_as_{0}mer_bpe_ws256_documents.txt'.format(k), 'doc2vec_{}bpe_128dim.model'.format(k))]\n",
    "\n",
    "\n",
    "    #trials = [('all_16s_as_{0}mer_bpe_documents.txt'.format(k), 'doc2vec_{}bpe.model'.format(k))]\n",
    "    for tup in trials:\n",
    "        corpus_file = tup[0]\n",
    "        outfile = tup[1]\n",
    "        corpus= TaggedLineDocument(corpus_file)\n",
    "\n",
    "        model = Doc2Vec(**args)\n",
    "\n",
    "        print(\"Building Vocab....\")\n",
    "        s = time.time()\n",
    "        model.build_vocab(corpus_file=corpus_file)\n",
    "        e = time.time() - s\n",
    "        print(\"Vocab built in {}\".format(e))\n",
    "        print(\"Training...\")\n",
    "        logger = EpochLogger()\n",
    "        model.train(documents=corpus,\n",
    "                    epochs=20,\n",
    "                   total_examples=model.corpus_count,\n",
    "                   total_words=n_words,\n",
    "                   callbacks=[logger])\n",
    "        model.save(outfile)\n",
    "        print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
