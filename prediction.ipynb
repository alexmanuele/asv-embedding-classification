{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python core\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import itertools as it\n",
    "\n",
    "#numpy/pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#language modeling: Gensim, sentencepiece\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim import utils\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "#Scikit learn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Tensorflow\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Bio python\n",
    "from Bio import SeqIO\n",
    "\n",
    "#Ipython\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop samples without labels, drop any samples with no otus (shouldnt be any but just in case)\n",
    "#drop otus that arent in any samples \n",
    "def clean_table(study_name):\n",
    "    print(study_name,\":\")\n",
    "    df = pd.read_table('microbiome_datasets/selected_datasets_joined/{}.tsv'.format(study_name),sep=',',low_memory=False)\n",
    "    print(\"Shape before cleaning: \", df.shape)\n",
    "    #Drop samples with no label.\n",
    "    drop = df.isna().any()\n",
    "    drop[[\"OTU_ID\", 'Sequence']] = False #Dont drop the sequence and OTU columns. Sequence has NaN for disease state.\n",
    "    df = df.loc[:, ~drop]\n",
    "    #drop rows with all zeros. There shouldnt be any but just in case.\n",
    "    df = df.loc[~(df[df.columns]==0).all(axis=1)]\n",
    "    #Drop columns with all zeros. again, there shouldnt be any but just in case.\n",
    "    df = df.loc[:, (df!=0).any(axis=0)]\n",
    "    print(\"Shape after cleaning: \", df.shape)\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sequence to kmers seperated by white space\n",
    "def seq_to_kmers(seq, k):\n",
    "    index = 0\n",
    "    out = \"\"\n",
    "    while index < len(seq) - k:\n",
    "        out += \"{0} \".format(seq[index: index + k])\n",
    "        index += 1\n",
    "    out += seq[index:index+k]\n",
    "    return out\n",
    "\n",
    "#Used during Doc2Vec training, must be defined here to surpress error.\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.time = None\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        self.time = time.time()\n",
    "    def on_epoch_end(self, model):\n",
    "        e = time.time() - self.time\n",
    "        print(\"Epoch #{0} end in {1}\".format(self.epoch, e))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set processing methods\n",
    "Note: Compute time could be saved by combining these methods. Keeping htem seperate, however, allows for individual analyses using any given method as well as comparison of run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qiime2 uses a simple norm so i'll do that for now.\n",
    "def relative_abundance_xy(df):\n",
    "    samples = df[[col for col in df.columns if col not in ['OTU_ID', 'Sequence']]].T.values\n",
    "    X = samples[:,:-1]\n",
    "    y = samples[:,-1]\n",
    "    \n",
    "    X = X.astype(np.float)\n",
    "    X = X / X.sum()\n",
    "    # return relative abundance matrix and y labels\n",
    "    return X, y\n",
    "\n",
    "\"\"\"\n",
    "df: the standard otu dataframe\n",
    "sp: sentencepiece model\n",
    "d2v: doc2vec model\n",
    "\n",
    "#encode all the sequences\n",
    "# for each sample, multiply \n",
    "\"\"\"\n",
    "def bpe_docs_xy(df, sp, d2v):\n",
    "    print(\"Encoding tokens...\")\n",
    "    bpe_encoded = [sp.encode_as_pieces(seq) for seq in df['Sequence'].values[:-1]]\n",
    "    print(\"Encoding vectors...\")\n",
    "    as_vectors = [d2v.infer_vector(seq) for seq in bpe_encoded]\n",
    "    print(\"Processing...\")\n",
    "    samples = [col for col in df.columns if col not in ['OTU_ID', \"Sequence\"]]\n",
    "    X = []\n",
    "    y = []\n",
    "    for sample in samples:\n",
    "        vecs = []\n",
    "        values = df[sample].values[:-1].astype(np.float)\n",
    "        values = values / values.sum()\n",
    "        for i, value in enumerate(values):\n",
    "            weighted_vec = value * as_vectors[i]\n",
    "            vecs.append(weighted_vec)\n",
    "        X.append(vecs)\n",
    "        y.append(df[sample].values[-1])\n",
    "    X = np.asarray(X).sum(axis=1)\n",
    "    print(\"Done.\")\n",
    "    return X, y\n",
    " \n",
    "def kmer_docs_xy(df, d2v, k):\n",
    "    print(\"Formatting sequences...\")\n",
    "    kmers = [seq_to_kmers(seq, k).split(' ') for seq in df['Sequence'].values[:-1]]\n",
    "    print(\"Encoding vectors...\")\n",
    "    as_vectors = [d2v.infer_vector(seq) for seq in kmers]\n",
    "    print(\"Processing...\")\n",
    "    samples = [col for col in df.columns if col not in ['OTU_ID', \"Sequence\"]]\n",
    "    X = []\n",
    "    y = []\n",
    "    for sample in samples:\n",
    "        vecs = []\n",
    "        values = df[sample].values[:-1].astype(np.float)\n",
    "        values = values / values.sum()\n",
    "        for i, value in enumerate(values):\n",
    "            weighted_vec = value * as_vectors[i]\n",
    "            vecs.append(weighted_vec)\n",
    "        X.append(vecs)\n",
    "        y.append(df[sample].values[-1])\n",
    "    X = np.asarray(X).sum(axis=1)\n",
    "    print(\"Done.\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels):\n",
    "    labels = np.asarray(labels).reshape(-1,1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(labels)\n",
    "    return ohe.transform(labels)\n",
    "\n",
    "def prepare_for_keras(X, y):\n",
    "    return np.reshape(X, (X.shape[0], X.shape[1], 1)), labels_to_onehot(y)\n",
    "\n",
    "\n",
    "# Create a simple CNN model.\n",
    "# Returns a single-layer 1D CNN with pooling and dropout.\n",
    "# Input param k is the embedding dimension.\n",
    "# TODO maybe allow param adjustment?\n",
    "def create_model(emb_dim):\n",
    "    inputs = layers.Input(shape=(emb_dim,1,))\n",
    "    x = layers.Conv1D(32, 2)(inputs)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(30, activation='relu')(x)\n",
    "    outputs = layers.Dense(2,activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method for adding noisy artificial data to the data set.\n",
    "i.e. gaussian noise data augmentaiton\n",
    "\n",
    "-- Params --\n",
    "x_data: The training data with labels removed\n",
    "y: The labels corresponding to x data\n",
    "magnitude: The number of fold of x_data to synthesize.\n",
    "           If magnitude == 1, len(x_data) will be syntheized.\n",
    "           If magnitude == 2, 2 * len(x_data) will be synthesized, etc.\n",
    "sigma: Standard deviation for gaussian distribution of noise centered around 0.\n",
    "emb)dim: cardinality of embedding dimension\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def gaussian_expansion(x_data,y, magnitude, sigma, emb_dim):\n",
    "    mu = 0.0\n",
    "    num_samples = x_data.shape[0]\n",
    "    x_noised = []\n",
    "    for i in range(magnitude):\n",
    "        noise = np.random.normal(mu, sigma, x_data.shape)\n",
    "        x_noised.append(x_data + noise)\n",
    "\n",
    "    y_new = [y] * (magnitude +1)\n",
    "    y_new = np.asarray(y_new).reshape(y.shape[0]*(magnitude+1), y.shape[1])\n",
    "    x_noised = np.asarray(x_noised)\n",
    "    x_noised = x_noised.reshape(magnitude*x_data.shape[0],emb_dim,1)\n",
    "    #y_new = enc.transform(np.reshape(y_new, (-1,1)))\n",
    "    new_x = np.concatenate((x_data, x_noised))\n",
    "\n",
    "    return(new_x, y_new)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method for running k-fold CV with neural network model.\n",
    "\n",
    "-- Params --\n",
    "X: The entire dataset, no labels.\n",
    "y: The labels.\n",
    "n_folds: Number of folds CV\n",
    "sigma: std dev for gaussian noise\n",
    "noise_size: number fold to increase data set with noise\n",
    "enc: sklearn OneHotEncoder object\n",
    "desc: Description dictionary to provide readable results\n",
    "emb_dim: number of features selected\n",
    "\"\"\"\n",
    "def kfold_cnn(X, y, \n",
    "              n_folds, \n",
    "              sigma_scale, noise_size, \n",
    "              emb_dim, \n",
    "              dataset_name,\n",
    "              processing_name,\n",
    "              otu_table=False):\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    scores = []\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(y.reshape(-1,1))\n",
    "    \n",
    "    #In each train/test split, split the train into train/validation.\n",
    "    for train, test in skf.split(X, y):\n",
    "        print(dataset_name, processing_name)\n",
    "        \n",
    "        X_new = X\n",
    "        #if its a count table, need to select K best. \n",
    "        if otu_table:\n",
    "            print(\"Reducing OTU table dimension...\")\n",
    "            kbest = SelectKBest(mutual_info_classif, k=emb_dim)\n",
    "            #only select K best from the train set.\n",
    "            kbest.fit(X[train], y[train])\n",
    "            X_new = kbest.transform(X)   \n",
    "            \n",
    "        X_new = np.reshape(X_new, (X_new.shape[0], X_new.shape[1], 1))\n",
    "        \n",
    "        X_train, X_test = X_new[train], X_new[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        #Need a validation set. Take 15% of the train data.\n",
    "        x_tr, x_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train)\n",
    "        \n",
    "        y_tr = ohe.transform(y_tr.reshape(-1,1))\n",
    "        y_val = ohe.transform(y_val.reshape(-1,1))\n",
    "        y_test = ohe.transform(y_test.reshape(-1,1))\n",
    "        \n",
    "        #Add gaussian noise. I've found this improves results.\n",
    "        # Instead of guessing sigma, get it from X_train.\n",
    "        sigma = X_train.flatten().std() * sigma_scale\n",
    "        x_tr, y_tr = gaussian_expansion(x_tr, y_tr, noise_size, sigma, 128)\n",
    "        \n",
    "        #Create model\n",
    "        model = create_model(emb_dim)\n",
    "        \n",
    "        #Fit model, using validation set for early stopping\n",
    "        model.fit(x_tr, y_tr, batch_size=6, epochs=200, validation_data=(x_val, y_val),\n",
    "                  callbacks=[EarlyStopping('val_loss', patience=35, restore_best_weights=True)])\n",
    "\n",
    "        evals = model.evaluate(X_test, y_test)\n",
    "        clear_output()\n",
    "        scores.append(evals[1])\n",
    "    return {'dataset': dataset_name,\n",
    "            'feature_repr': processing_name,\n",
    "            'n_features': emb_dim,\n",
    "            'noise_sigma': sigma,\n",
    "            'noise_magnitude': noise_size,\n",
    "            'cv_n_fold': n_folds,\n",
    "            'mean_acc': np.mean(scores),\n",
    "            'mean_std': np.std(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cnn(X, y, \n",
    "              n_folds, \n",
    "              sigma_scale, noise_size, \n",
    "              emb_dim, \n",
    "              dataset_name,\n",
    "              processing_name,\n",
    "              otu_table=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'cdi_youngster' removed since it apparenlty is not balanced at all.\n",
    "studies = ['asd_son',  'crc_baxter', 'crc_zeller', 'hiv_lozupone', 'hiv_noguerajulian',\n",
    "          'ibd_gevers_2014', 'ob_goodrich', 't1d_alkanani']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "K = [6,8]\n",
    "cv_fold = 2\n",
    "embed_dim = 128\n",
    "##########################\n",
    "\n",
    "noise_params = [(1, 0),\n",
    "                (1, 10),\n",
    "                (1, 100),\n",
    "                (1, 500),\n",
    "                (10, 10),\n",
    "                (10, 100),\n",
    "                (10, 500),\n",
    "                (100, 10),\n",
    "                (100, 100),\n",
    "                (100, 500)]\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for study in studies:\n",
    "    df = clean_table(study)\n",
    "    #otu trials\n",
    "    otu_desc = 'otu_relative_abundance'\n",
    "    X, y = relative_abundance_xy(df)\n",
    "    for params in noise_params:\n",
    "        all_results.append(\n",
    "            kfold_cnn(X, y, cv_fold, params[0], params[1], embed_dim, study, otu_desc, otu_table=True)\n",
    "        )\n",
    "    #kmer and BPE\n",
    "    for k in K:\n",
    "        #kmers first\n",
    "        kmer_desc = 'kmer_size_{}_document_vectors'.format(k)\n",
    "        d2v = Doc2Vec.load('doc2vecmodels/doc2vec_{}mers_128dim.model'.format(k))\n",
    "        \n",
    "        X, y = kmer_docs_xy(df, d2v, k)\n",
    "        for params in noise_params:\n",
    "            all_results.append(\n",
    "                kfold_cnn(X, y, cv_fold, params[0], params[1], embed_dim, study, kmer_desc, otu_table=False)\n",
    "            )\n",
    "        \n",
    "        #bpe \n",
    "        bpe_desc = 'bpe_size_{}_document_vectors'.format(k)\n",
    "        d2v = Doc2Vec.load('doc2vecmodels/doc2vec_{}bpe_128dim.model'.format(k))\n",
    "        #sentencpiece\n",
    "        SP_FILE = 'bpe_models/{}mer_compare_bpe_dna_wordsize_256.model'.format(k)\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(SP_FILE)\n",
    "        \n",
    "        X, y = bpe_docs_xy(df, sp, d2v)\n",
    "        for params in noise_params:\n",
    "            all_results.append(\n",
    "                kfold_cnn(X, y, cv_fold, params[0], params[1], embed_dim, study, bpe_desc, otu_table=False)\n",
    "            )\n",
    "\n",
    "del(X)\n",
    "del(y)\n",
    "df = pd.DataFrame.from_records(all_results)\n",
    "df.to_csv('experiment_results.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
